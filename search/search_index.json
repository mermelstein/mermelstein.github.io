{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome! I'm Daniel Mermelstein, currently Director of Data at Sylva. About Me My career has been a journey through various parts of the data spectrum: History degree \u2192 10-person startup \u2192 go-to (AKA only) business intelligence analyst at a global non-profit \u2192 data analyst (adtech) \u2192 data analyst / data scientist (fintech) \u2192 data engineer \u2192 data team leader. Fun Side Projects: ( See All Projects \u00bb ) Over the years my project work has grown across a few areas. Below are some highlights I've published or am able to discuss publicly. My professional experience also includes: Leading high-performing data teams Automating accounting processes at a holding company Closing a consumer bank (you should really ask about this one!) Building A/B test analysis platforms, data enrichment pipelines (e.g. from LinkedIn), and automated reporting solutions for product, marketing, finance, accounting, and general BI. DWH migrations or implementing a DWH solution from 0 to 1 Designing and implementing systems for Retrieval Augmented Generation (RAG) with PostgreSQL and pgvector Data Engineering & Orchestration Open-source dbt runner using GitHub Actions: Simple dbt Runner R package for easy database querying (Snowflake, Redshift, Postgres): Snowquery R & Python in one Docker Container: docker-python-and-r DuckDB Simultaneous Connection Benchmark: duckdb_connection_benchmark Web Applications & Data Visualization Searchable database of all the 3K programs in NYC: NYC3K.com Automated surf forecasts for the Hudson River: Downwinder Sportsbetting data and NBA game predictions: BloomBet Terminal Personalized rowing workouts: Row Hard AI, Language & Development Tools VSCode plugin for a custom LLM chat tool: e/acc Copilot Haggadah Translation (English to Mandarin): Haggadah Translation Beyond my professional life, I'm an avid extracurricular athlete and made it to the final round of selection for Team USA in 2015. I am a 5x Bronze medalist at the Dragon Boat World Championships.","title":"Home"},{"location":"#welcome","text":"I'm Daniel Mermelstein, currently Director of Data at Sylva.","title":"Welcome!"},{"location":"#about-me","text":"My career has been a journey through various parts of the data spectrum: History degree \u2192 10-person startup \u2192 go-to (AKA only) business intelligence analyst at a global non-profit \u2192 data analyst (adtech) \u2192 data analyst / data scientist (fintech) \u2192 data engineer \u2192 data team leader. Fun Side Projects: ( See All Projects \u00bb ) Over the years my project work has grown across a few areas. Below are some highlights I've published or am able to discuss publicly. My professional experience also includes: Leading high-performing data teams Automating accounting processes at a holding company Closing a consumer bank (you should really ask about this one!) Building A/B test analysis platforms, data enrichment pipelines (e.g. from LinkedIn), and automated reporting solutions for product, marketing, finance, accounting, and general BI. DWH migrations or implementing a DWH solution from 0 to 1 Designing and implementing systems for Retrieval Augmented Generation (RAG) with PostgreSQL and pgvector","title":"About Me"},{"location":"projects/","text":"Projects Here are some of the side projects I've worked on over the years. Downwinder: Hudson River Surf Forecast Description: An automated system that fetches wind and tide data to forecast surfable wave conditions on the Hudson River. The forecasts are published to a Google Sheet. Technologies: Python, Docker, PostgreSQL, OpenWeatherMap API, NOAA Tides & Currents API, Google Sheets API. Status: Active ( GSheet link ). Read more about the Downwinder project \u00bb NYC 3K Finder Description: A web app to help parents find NYC's 3K (early childhood education) programs. It uses scraped public data, address geocoding, and distance calculations to display nearby schools and a child's admission likelihood. Technologies: Python (Django), Docker, Photon API (address autocomplete), Nominatim API (geocoding). Status: Active ( NYC3K.com ). Read more about the NYC 3K Finder project \u00bb Snowquery: R Package for Database Querying Description: An R package to simplify running SQL queries on Snowflake, Redshift, and PostgreSQL. It leverages the Snowflake Python connector under the hood for robust Snowflake access. It allows easy data querying directly from R scripts without complex setup. Technologies: R, Python, reticulate, DBI, RPostgres, Snowflake Python Connector. Status: Active ( CRAN , GitHub , Website ). Read more about the Snowquery project \u00bb Simple dbt Runner: Self-Host dbt with GitHub Actions Description: An open-source project with GitHub Actions workflows to run dbt in production. I was the lead contributor, we built a cost-effective alternative to dbt Cloud for self-hosting dbt orchestration, CI/CD, and documentation. Technologies: dbt, GitHub Actions, Python, YAML, Shell. Status: Active, Open-Source ( GitHub ). Read more about the Simple dbt Runner project \u00bb Haggadah Translation Description: A project to translate the Goldberg Haggadah from English to Chinese. It uses Python and OCR to extract the English text from a PDF, then uses GPT-3.5 to translate the text page by page. Technologies: Python, Docker, OCR (pytesseract), GPT-3.5. Status: Complete Read more about the Haggadah Translation project \u00bb","title":"Overview"},{"location":"projects/#projects","text":"Here are some of the side projects I've worked on over the years.","title":"Projects"},{"location":"projects/#downwinder-hudson-river-surf-forecast","text":"Description: An automated system that fetches wind and tide data to forecast surfable wave conditions on the Hudson River. The forecasts are published to a Google Sheet. Technologies: Python, Docker, PostgreSQL, OpenWeatherMap API, NOAA Tides & Currents API, Google Sheets API. Status: Active ( GSheet link ). Read more about the Downwinder project \u00bb","title":"Downwinder: Hudson River Surf Forecast"},{"location":"projects/#nyc-3k-finder","text":"Description: A web app to help parents find NYC's 3K (early childhood education) programs. It uses scraped public data, address geocoding, and distance calculations to display nearby schools and a child's admission likelihood. Technologies: Python (Django), Docker, Photon API (address autocomplete), Nominatim API (geocoding). Status: Active ( NYC3K.com ). Read more about the NYC 3K Finder project \u00bb","title":"NYC 3K Finder"},{"location":"projects/#snowquery-r-package-for-database-querying","text":"Description: An R package to simplify running SQL queries on Snowflake, Redshift, and PostgreSQL. It leverages the Snowflake Python connector under the hood for robust Snowflake access. It allows easy data querying directly from R scripts without complex setup. Technologies: R, Python, reticulate, DBI, RPostgres, Snowflake Python Connector. Status: Active ( CRAN , GitHub , Website ). Read more about the Snowquery project \u00bb","title":"Snowquery: R Package for Database Querying"},{"location":"projects/#simple-dbt-runner-self-host-dbt-with-github-actions","text":"Description: An open-source project with GitHub Actions workflows to run dbt in production. I was the lead contributor, we built a cost-effective alternative to dbt Cloud for self-hosting dbt orchestration, CI/CD, and documentation. Technologies: dbt, GitHub Actions, Python, YAML, Shell. Status: Active, Open-Source ( GitHub ). Read more about the Simple dbt Runner project \u00bb","title":"Simple dbt Runner: Self-Host dbt with GitHub Actions"},{"location":"projects/#haggadah-translation","text":"Description: A project to translate the Goldberg Haggadah from English to Chinese. It uses Python and OCR to extract the English text from a PDF, then uses GPT-3.5 to translate the text page by page. Technologies: Python, Docker, OCR (pytesseract), GPT-3.5. Status: Complete Read more about the Haggadah Translation project \u00bb","title":"Haggadah Translation"},{"location":"projects/downwinder/","text":"Downwinder: Hudson River Surf Forecast After many years dragonboating I bought an outrigger canoe and started paddling on the Hudson. Primarily for my own fitness, but I quickly discovered that out in the center of the river, if the conditions were right, you could surf! Joy oh joy! I've seen waves of 2-4 feet out there. The problem was that it took about an hour to get from my apartment to the boathouse, plus another 20 minutes to set up the canoe, only to find that the wind had shifted and there weren't any waves. That's where this forecaster came into play -> it's accurate to about 1/4 of a foot. And worse-case scenario I can always just surf the wake of the container ships. Overview The \"Downwinder\" service is a Google Sheet that's backed by a python service that does the following: Fetches current and forecast water speed data from the NOAA Tides & Currents API. Fetches wind forecast data (speed and direction) from the OpenWeatherMap API. Combines these datasets, aligning them by timestamp. Calculates the effective wind speed acting on the water surface, considering the direction of water flow. Estimates potential wave heights using the Bretschneider's formula, based on the projected wind effect. Stores the raw and processed data in a cloud-hosted PostgreSQL database. Publishes a user-friendly forecast to a public Google Sheet . It was easy to check on my phone before heading out. Posts updates and notable conditions to a dedicated Twitter account (It used to do this before Elon shut down the public Twitter API). Technical Stack Programming Language: Python Data Manipulation: Pandas, NumPy APIs: NOAA Tides & Currents API (water data) OpenWeatherMap API (wind data) Google Sheets API (publishing forecasts) Twitter API (social media updates) Database: PostgreSQL (cloud-hosted, will probably self-host at some point) Deployment & Orchestration: Docker for containerizing the Python application. A mac mini sitting on my desk. A cron job that runs the service once an hour. How It Works The fundamental idea is that waves can form when wind blows over water; if the water has a current then an opposing wind can be gentle and still apply force to the surface. The Hudson River exhibits this phenomenon because it has strong tidal currents. So what I do here is automate the process of: Gathering Data: Regularly fetching wind forecasts (speed and direction) and water current forecasts (speed and direction). Aligning Forces: The core challenge is a geometric one: determining how the wind's force aligns with the water's movement over a sufficient distance (fetch) for waves to develop. We need to calculate the component of wind that is directly opposing or running with the current. Estimating Wave Formation: Once the effective wind acting on the water is quantified, a mathematical model estimates the potential wave height. Publishing Results: The forecast is then made available through a Google Sheet. Wave Height Estimation: A Tale of Two Formulas Estimating wave height in a dynamic river environment is not as complex as it seems. My initial approach involved a formula from the paper Wave Forecasting in Shallow Water: A New Set of Growth Curves Depending on Bed Roughness . This seemed promising as it was designed for shallow water. However, it was primarily focused on depths less than 4 meters and fetch distances (the length of water over which wind blows) of around 1 kilometer. These parameters didn't quite match the typical conditions or the shorter fetch distances relevant for quick wave formation on the Hudson. Interestingly what worked better was the Bretschneider formula , which was developed in 1976 for forecasting waves generated by hurricanes in the open ocean ( paper here ). With some eyeballing and empirical adjustments that gave me reasonably accurate estimates for the Hudson over fetch distances as short as 50-100 meters. The key data transformation now was to resolve the wind and water vectors to understand the direct opposition or reinforcement, which then feeds into my adjusted Bretschneider model. We do a little bit of trigonometry to find the effective wind speed in the direction of the water flow and BAM! we have a wave height estimate. Now we know if it's surf time. Output & Publishing The processed forecast, including estimated wave heights, is written to a database. A subset of that data is then pushed to the public Google Sheet for easy viewing. Key forecast highlights or alerts used to be sent out via Twitter before the API changes. You can view the live forecast here: ( Hudson River Downwind Forecast Sheet )","title":"Downwinder Forecast"},{"location":"projects/downwinder/#downwinder-hudson-river-surf-forecast","text":"After many years dragonboating I bought an outrigger canoe and started paddling on the Hudson. Primarily for my own fitness, but I quickly discovered that out in the center of the river, if the conditions were right, you could surf! Joy oh joy! I've seen waves of 2-4 feet out there. The problem was that it took about an hour to get from my apartment to the boathouse, plus another 20 minutes to set up the canoe, only to find that the wind had shifted and there weren't any waves. That's where this forecaster came into play -> it's accurate to about 1/4 of a foot. And worse-case scenario I can always just surf the wake of the container ships.","title":"Downwinder: Hudson River Surf Forecast"},{"location":"projects/downwinder/#overview","text":"The \"Downwinder\" service is a Google Sheet that's backed by a python service that does the following: Fetches current and forecast water speed data from the NOAA Tides & Currents API. Fetches wind forecast data (speed and direction) from the OpenWeatherMap API. Combines these datasets, aligning them by timestamp. Calculates the effective wind speed acting on the water surface, considering the direction of water flow. Estimates potential wave heights using the Bretschneider's formula, based on the projected wind effect. Stores the raw and processed data in a cloud-hosted PostgreSQL database. Publishes a user-friendly forecast to a public Google Sheet . It was easy to check on my phone before heading out. Posts updates and notable conditions to a dedicated Twitter account (It used to do this before Elon shut down the public Twitter API).","title":"Overview"},{"location":"projects/downwinder/#technical-stack","text":"Programming Language: Python Data Manipulation: Pandas, NumPy APIs: NOAA Tides & Currents API (water data) OpenWeatherMap API (wind data) Google Sheets API (publishing forecasts) Twitter API (social media updates) Database: PostgreSQL (cloud-hosted, will probably self-host at some point) Deployment & Orchestration: Docker for containerizing the Python application. A mac mini sitting on my desk. A cron job that runs the service once an hour.","title":"Technical Stack"},{"location":"projects/downwinder/#how-it-works","text":"The fundamental idea is that waves can form when wind blows over water; if the water has a current then an opposing wind can be gentle and still apply force to the surface. The Hudson River exhibits this phenomenon because it has strong tidal currents. So what I do here is automate the process of: Gathering Data: Regularly fetching wind forecasts (speed and direction) and water current forecasts (speed and direction). Aligning Forces: The core challenge is a geometric one: determining how the wind's force aligns with the water's movement over a sufficient distance (fetch) for waves to develop. We need to calculate the component of wind that is directly opposing or running with the current. Estimating Wave Formation: Once the effective wind acting on the water is quantified, a mathematical model estimates the potential wave height. Publishing Results: The forecast is then made available through a Google Sheet.","title":"How It Works"},{"location":"projects/downwinder/#wave-height-estimation-a-tale-of-two-formulas","text":"Estimating wave height in a dynamic river environment is not as complex as it seems. My initial approach involved a formula from the paper Wave Forecasting in Shallow Water: A New Set of Growth Curves Depending on Bed Roughness . This seemed promising as it was designed for shallow water. However, it was primarily focused on depths less than 4 meters and fetch distances (the length of water over which wind blows) of around 1 kilometer. These parameters didn't quite match the typical conditions or the shorter fetch distances relevant for quick wave formation on the Hudson. Interestingly what worked better was the Bretschneider formula , which was developed in 1976 for forecasting waves generated by hurricanes in the open ocean ( paper here ). With some eyeballing and empirical adjustments that gave me reasonably accurate estimates for the Hudson over fetch distances as short as 50-100 meters. The key data transformation now was to resolve the wind and water vectors to understand the direct opposition or reinforcement, which then feeds into my adjusted Bretschneider model. We do a little bit of trigonometry to find the effective wind speed in the direction of the water flow and BAM! we have a wave height estimate. Now we know if it's surf time.","title":"Wave Height Estimation: A Tale of Two Formulas"},{"location":"projects/downwinder/#output-publishing","text":"The processed forecast, including estimated wave heights, is written to a database. A subset of that data is then pushed to the public Google Sheet for easy viewing. Key forecast highlights or alerts used to be sent out via Twitter before the API changes. You can view the live forecast here: ( Hudson River Downwind Forecast Sheet )","title":"Output &amp; Publishing"},{"location":"projects/haggadah_translation/","text":"Project: Haggadah Translation While my in-laws were visiting for Passover in 2023, I wanted to ensure they could fully participate in the Seder. Problems was they don't speak English very well. To help them follow along I translated our family's Haggadah into Mandarin Chinese and give them their own copy. The Process The goal was to create a translated version that mirrored the original's layout and page numbering, making it easy for everyone to follow along (we even had them read from their book when it was their turn). Source Material: I found a PDF version of our specific Haggadah. Text Extraction: I extracted the English text from the PDF, one page at a time using off-the-shelf OCR software. Translation: Each page's text was then fed into GPT-3.5 with a prompt to translate it into Mandarin Chinese while preserving the original structure. Quality Assurance: A native Mandarin speaker (my wife) reviewed the translation. They gave it a thumbs-up, confirming accuracy and readability. Final Product: The translated pages were compiled and printed to create a complete, parallel Haggadah for the in-laws. Technical Stack Python: For scripting the OCR and translation workflow. OCR (pytesseract): To extract text from the PDF. GPT-3.5: For the English to Mandarin Chinese translation. Project Status Status: Completed.","title":"Haggadah Translation"},{"location":"projects/haggadah_translation/#project-haggadah-translation","text":"While my in-laws were visiting for Passover in 2023, I wanted to ensure they could fully participate in the Seder. Problems was they don't speak English very well. To help them follow along I translated our family's Haggadah into Mandarin Chinese and give them their own copy.","title":"Project: Haggadah Translation"},{"location":"projects/haggadah_translation/#the-process","text":"The goal was to create a translated version that mirrored the original's layout and page numbering, making it easy for everyone to follow along (we even had them read from their book when it was their turn). Source Material: I found a PDF version of our specific Haggadah. Text Extraction: I extracted the English text from the PDF, one page at a time using off-the-shelf OCR software. Translation: Each page's text was then fed into GPT-3.5 with a prompt to translate it into Mandarin Chinese while preserving the original structure. Quality Assurance: A native Mandarin speaker (my wife) reviewed the translation. They gave it a thumbs-up, confirming accuracy and readability. Final Product: The translated pages were compiled and printed to create a complete, parallel Haggadah for the in-laws.","title":"The Process"},{"location":"projects/haggadah_translation/#technical-stack","text":"Python: For scripting the OCR and translation workflow. OCR (pytesseract): To extract text from the PDF. GPT-3.5: For the English to Mandarin Chinese translation.","title":"Technical Stack"},{"location":"projects/haggadah_translation/#project-status","text":"Status: Completed.","title":"Project Status"},{"location":"projects/nyc3k/","text":"Project: NYC 3K Finder I developed the NYC 3K Finder to help my family navigate the complex process of finding 3K programs for our kids. My goal was to consolidate information and allows parents to easily explore nearby options. The 3K Data 3K is the New York City public funding program for children 3 years old. Originally meant to cover all children in the city, it has been reduced in scope and the slots are limited to specific areas and schools/daycares. It can save parents around $20,000 per year compared to private daycare options. This means it can be extremely competitive to get a spot for your child. First I needed to scrape and process publicly available data from the NYC Department of Education and MySchools.nyc. That got me: The number of allocated 3K spots per school How each school prioritizes applications (eg current students > siblings of current students > children living near the school > other children in the borough, etc) A categorization for how many applicants in each priority group were offered a seat in the previous year The official name and address of each school The official website for each school The type of 3K program offered (e.g., Full Day, Half Day, etc.) Other data That's enough to get started. The goal was to process the data in a way that made it easier to search for programs near us and understand whether our children would be likely to get a in if we applied. This turned into in a categorical score for each program and each priority group (eg \"High Likelihood\" for current students, \"Medium Likelihood\" for siblings, etc). The Tool For each program, the tool displays: School Name & Program Type: The official name and the specific 3K program. Address: The physical location. School Website: A direct link to the program's official site, where available. MySchools.nyc Link: A direct link to the program page on MySchools for deeper research. 3K Spots Available: The reported number of 3K seats for the most recent available year. Application Success Rate (ASR): A breakdown of how many applicants in each priority group were offered a seat in the previous cycle, categorized as High, Medium, Low Likelihood, or No Applicants. Application Success Rate A main feature from a data and usability perspective is the ASR. This gave us an idea of a program's competitiveness for different applicant groups based on historical admission data. It wasn't a guarantee of admission, but it helped us prioritize our applications and understand our chances of getting in. The ASR is categorized as: High Likelihood: All applicants in this category who listed the program were offered a seat in the previous year. Medium Likelihood: Some applicants in this category were offered a seat. Low Likelihood: No applicants in this group were offered a seat. No Applicants: No one in the listed group applied for the program in the previous cycle. Not every school has admissions data by priority group but most did and it helped clarify the competitive landscape. How the Finder Works Program Search: Users can input their home address. The system then queries and displays 3K programs within a 1-mile radius on an interactive map. The backend uses Photon for address auto-complete (both while you type and also after search submission to have a more complete address) and we feed that address to Nominatim for lat/long coordinates. Then a quick application of the Haversine formula gets us distances of all schools to our searched address and we filter for those within 1 mile. Interactive Map: Nearby programs are represented by blue map pins. Clicking a pin reveals a pop-up with details: name, address, website, available spots, and the ASR. I also integrated a \"Directions\" link to Google Maps for convenience. Filtering Capabilities: Once results are shown, a filter panel allows users to refine their search. Users can filter programs based on where specific applicant groups had a \"High Likelihood\" of admission in the previous year (e.g., Sibling Priority). Project Links & Status Status: Active. GitHub Repository: https://nyc3k.com","title":"NYC 3K Finder"},{"location":"projects/nyc3k/#project-nyc-3k-finder","text":"I developed the NYC 3K Finder to help my family navigate the complex process of finding 3K programs for our kids. My goal was to consolidate information and allows parents to easily explore nearby options.","title":"Project: NYC 3K Finder"},{"location":"projects/nyc3k/#the-3k-data","text":"3K is the New York City public funding program for children 3 years old. Originally meant to cover all children in the city, it has been reduced in scope and the slots are limited to specific areas and schools/daycares. It can save parents around $20,000 per year compared to private daycare options. This means it can be extremely competitive to get a spot for your child. First I needed to scrape and process publicly available data from the NYC Department of Education and MySchools.nyc. That got me: The number of allocated 3K spots per school How each school prioritizes applications (eg current students > siblings of current students > children living near the school > other children in the borough, etc) A categorization for how many applicants in each priority group were offered a seat in the previous year The official name and address of each school The official website for each school The type of 3K program offered (e.g., Full Day, Half Day, etc.) Other data That's enough to get started. The goal was to process the data in a way that made it easier to search for programs near us and understand whether our children would be likely to get a in if we applied. This turned into in a categorical score for each program and each priority group (eg \"High Likelihood\" for current students, \"Medium Likelihood\" for siblings, etc).","title":"The 3K Data"},{"location":"projects/nyc3k/#the-tool","text":"For each program, the tool displays: School Name & Program Type: The official name and the specific 3K program. Address: The physical location. School Website: A direct link to the program's official site, where available. MySchools.nyc Link: A direct link to the program page on MySchools for deeper research. 3K Spots Available: The reported number of 3K seats for the most recent available year. Application Success Rate (ASR): A breakdown of how many applicants in each priority group were offered a seat in the previous cycle, categorized as High, Medium, Low Likelihood, or No Applicants.","title":"The Tool"},{"location":"projects/nyc3k/#application-success-rate","text":"A main feature from a data and usability perspective is the ASR. This gave us an idea of a program's competitiveness for different applicant groups based on historical admission data. It wasn't a guarantee of admission, but it helped us prioritize our applications and understand our chances of getting in. The ASR is categorized as: High Likelihood: All applicants in this category who listed the program were offered a seat in the previous year. Medium Likelihood: Some applicants in this category were offered a seat. Low Likelihood: No applicants in this group were offered a seat. No Applicants: No one in the listed group applied for the program in the previous cycle. Not every school has admissions data by priority group but most did and it helped clarify the competitive landscape.","title":"Application Success Rate"},{"location":"projects/nyc3k/#how-the-finder-works","text":"","title":"How the Finder Works"},{"location":"projects/nyc3k/#program-search","text":"Users can input their home address. The system then queries and displays 3K programs within a 1-mile radius on an interactive map. The backend uses Photon for address auto-complete (both while you type and also after search submission to have a more complete address) and we feed that address to Nominatim for lat/long coordinates. Then a quick application of the Haversine formula gets us distances of all schools to our searched address and we filter for those within 1 mile.","title":"Program Search:"},{"location":"projects/nyc3k/#interactive-map","text":"Nearby programs are represented by blue map pins. Clicking a pin reveals a pop-up with details: name, address, website, available spots, and the ASR. I also integrated a \"Directions\" link to Google Maps for convenience.","title":"Interactive Map:"},{"location":"projects/nyc3k/#filtering-capabilities","text":"Once results are shown, a filter panel allows users to refine their search. Users can filter programs based on where specific applicant groups had a \"High Likelihood\" of admission in the previous year (e.g., Sibling Priority).","title":"Filtering Capabilities:"},{"location":"projects/nyc3k/#project-links-status","text":"Status: Active. GitHub Repository: https://nyc3k.com","title":"Project Links &amp; Status"},{"location":"projects/simple-dbt-runner/","text":"Project: Simple dbt Runner - Self-Host dbt with GitHub Actions The \"Simple dbt Runner\" is an open-source project I spearheaded to provide a cost-effective and flexible alternative for running dbt (data build tool) in production. This was a response to changes in dbt Cloud's pricing, which for my organization and many others would have resulted in cost increases of many multiples. The basics of the price change were from a cost per seat to usage costs based on time of each DAG run. The primary function of dbt Cloud at the time was just scheduling. There were a couple other features but nothing major, and we were happy to pay a small monthly fee to not have to think about it. the moment it became a real cost center we decided to cut it. Our Solution: Open-Source dbt Runners The Github repo offers a suite of pre-configured GitHub Actions workflows that allow teams to manage their dbt runs, CI/CD processes, and documentation hosting directly within the GitHub ecosystem. For most teams it would probably run in the free tier (Github Actions charges by runtime but the free tier threshold is reasonable). Key capabilities include: Scheduled Production Runs: Execute dbt jobs on a cron schedule. Merge-Triggered Runs: Full Runs: Execute all dbt models after a PR is merged to the main branch. State-Aware Incremental Runs: Intelligently run only modified models and their downstream dependencies, using a manifest.json stored in a gh-pages branch for state comparison. Continuous Integration (CI): Run dbt builds and tests on every commit to a pull request to ensure changes are valid before merging. dbt Documentation Hosting: Automatically generate and deploy dbt documentation to GitHub Pages, also utilizing the gh-pages branch. We started off with the assumption that users would want to host documentation privately and we selected AWS S3, but decided it was an additional complexity for this project. Large enough companies have Github Enterprise and are able to host documentation privately via Github Pages. How It Works The core idea is to provide a \"plug-and-play\" setup: Fork & Configure: Teams fork the repository and place their dbt project into a designated folder. Secrets Management: Database credentials and a GitHub Personal Access Token (for workflow permissions) are stored as GitHub Actions secrets. Automated Setup: A \"Project Setup\" workflow assists in configuring profiles.yml and requirements.txt for the target database. Workflow Customization: Users can select, modify, or delete the provided GitHub Action workflow files ( .yml ) to suit their specific needs. The state-aware incremental runs are a key feature, minimizing computation by only processing what's changed. This is achieved by comparing the current dbt project state against the manifest.json from the previous successful production run. Impact and Goals Reduced dbt costs at my organization by over 95% compared to dbt Cloud. Provided a robust, free, and open-source alternative to paid dbt orchestration services. Gave data teams full control over their dbt deployment environment and schedule. Simplified the process of setting up CI/CD for dbt projects. Technical Stack dbt (data build tool) GitHub Actions (for orchestration and CI/CD) Python (for dbt execution and utility scripts) YAML (for GitHub Actions workflow definitions) Shell scripting (for helper utilities) Project Links & Status Status: Active, open-source. GitHub Repository: simple-dbt-runner","title":"Simple dbt Runner"},{"location":"projects/simple-dbt-runner/#project-simple-dbt-runner-self-host-dbt-with-github-actions","text":"The \"Simple dbt Runner\" is an open-source project I spearheaded to provide a cost-effective and flexible alternative for running dbt (data build tool) in production. This was a response to changes in dbt Cloud's pricing, which for my organization and many others would have resulted in cost increases of many multiples. The basics of the price change were from a cost per seat to usage costs based on time of each DAG run. The primary function of dbt Cloud at the time was just scheduling. There were a couple other features but nothing major, and we were happy to pay a small monthly fee to not have to think about it. the moment it became a real cost center we decided to cut it.","title":"Project: Simple dbt Runner - Self-Host dbt with GitHub Actions"},{"location":"projects/simple-dbt-runner/#our-solution-open-source-dbt-runners","text":"The Github repo offers a suite of pre-configured GitHub Actions workflows that allow teams to manage their dbt runs, CI/CD processes, and documentation hosting directly within the GitHub ecosystem. For most teams it would probably run in the free tier (Github Actions charges by runtime but the free tier threshold is reasonable). Key capabilities include: Scheduled Production Runs: Execute dbt jobs on a cron schedule. Merge-Triggered Runs: Full Runs: Execute all dbt models after a PR is merged to the main branch. State-Aware Incremental Runs: Intelligently run only modified models and their downstream dependencies, using a manifest.json stored in a gh-pages branch for state comparison. Continuous Integration (CI): Run dbt builds and tests on every commit to a pull request to ensure changes are valid before merging. dbt Documentation Hosting: Automatically generate and deploy dbt documentation to GitHub Pages, also utilizing the gh-pages branch. We started off with the assumption that users would want to host documentation privately and we selected AWS S3, but decided it was an additional complexity for this project. Large enough companies have Github Enterprise and are able to host documentation privately via Github Pages.","title":"Our Solution: Open-Source dbt Runners"},{"location":"projects/simple-dbt-runner/#how-it-works","text":"The core idea is to provide a \"plug-and-play\" setup: Fork & Configure: Teams fork the repository and place their dbt project into a designated folder. Secrets Management: Database credentials and a GitHub Personal Access Token (for workflow permissions) are stored as GitHub Actions secrets. Automated Setup: A \"Project Setup\" workflow assists in configuring profiles.yml and requirements.txt for the target database. Workflow Customization: Users can select, modify, or delete the provided GitHub Action workflow files ( .yml ) to suit their specific needs. The state-aware incremental runs are a key feature, minimizing computation by only processing what's changed. This is achieved by comparing the current dbt project state against the manifest.json from the previous successful production run.","title":"How It Works"},{"location":"projects/simple-dbt-runner/#impact-and-goals","text":"Reduced dbt costs at my organization by over 95% compared to dbt Cloud. Provided a robust, free, and open-source alternative to paid dbt orchestration services. Gave data teams full control over their dbt deployment environment and schedule. Simplified the process of setting up CI/CD for dbt projects.","title":"Impact and Goals"},{"location":"projects/simple-dbt-runner/#technical-stack","text":"dbt (data build tool) GitHub Actions (for orchestration and CI/CD) Python (for dbt execution and utility scripts) YAML (for GitHub Actions workflow definitions) Shell scripting (for helper utilities)","title":"Technical Stack"},{"location":"projects/simple-dbt-runner/#project-links-status","text":"Status: Active, open-source. GitHub Repository: simple-dbt-runner","title":"Project Links &amp; Status"},{"location":"projects/snowquery/","text":"Project: Snowquery - R Package for Database Querying snowquery is an R package I developed to simplify running SQL queries against Snowflake, Redshift, and PostgreSQL databases directly from R. The Motivation I love R for data analysis. I chose Snowflake as the data warehouse for my startup. Snowflake only had a python connector. Nail: meet Hammer. I wanted: A unified package for querying databases we were using from R. Abstract away the complexities and python requirements for the Snowflake connection. A simple and local credential management system. How It Works For Snowflake: snowquery uses the reticulate package to run the official Snowflake Python connector in the background. This bypasses the need for users to manually configure ODBC drivers or deal with other R-specific Snowflake connection hurdles. The results come back as Pandas DataFrames from the Python connector which are seamlessly converted into R dataframes. For Redshift & PostgreSQL: For these databases, snowquery uses the standard DBI interface with the RPostgres driver, making for a normal and efficient connection method. Key Features Simplified Querying: A single function, queryDB() , is the main entry point for executing queries. Credential Management: The package looks for a snowquery_creds.yaml file in the user's home directory ( ~ ). This file can store connection details (account, username, password, warehouse, role, etc.) for multiple named connections. Credentials can also be passed directly to the queryDB() function, overriding or supplementing the YAML file. Supported Databases: Snowflake, Redshift, and PostgreSQL. Python Dependency Management: For Snowflake connections, it checks for a Python installation and the snowflake-connector-python[pandas] library, providing guidance through error messages if they are missing. Connection Timeout: Users can specify a timeout for database connections. Kind of important if you want to run very long-running queries. Technical Stack R Python (for Snowflake via reticulate ) snowflake-connector-python[pandas] DBI RPostgres yaml (for credential file parsing) Installation & Usage The package can be installed from CRAN or the development version from GitHub: # From CRAN install.packages(\"snowquery\") Typical usage would look like this: library(snowquery) # Using a named connection from snowquery_creds.yaml my_data <- queryDB(\"SELECT * FROM my_awesome_table\", conn_name = 'my_snowflake_dwh') # Manually passing credentials more_data <- queryDB( \"SELECT * FROM another_table\", db_type = 'postgres', host = 'localhost', port = 5432, database = 'mydb', username = 'user', password = 'password' ) Project Links & Status Status: Active and available on CRAN. CRAN: https://cran.r-project.org/package=snowquery GitHub Repository: https://github.com/mermelstein/snowquery Package Website: https://snowquery.org","title":"Snowquery"},{"location":"projects/snowquery/#project-snowquery-r-package-for-database-querying","text":"snowquery is an R package I developed to simplify running SQL queries against Snowflake, Redshift, and PostgreSQL databases directly from R.","title":"Project: Snowquery - R Package for Database Querying"},{"location":"projects/snowquery/#the-motivation","text":"I love R for data analysis. I chose Snowflake as the data warehouse for my startup. Snowflake only had a python connector. Nail: meet Hammer. I wanted: A unified package for querying databases we were using from R. Abstract away the complexities and python requirements for the Snowflake connection. A simple and local credential management system.","title":"The Motivation"},{"location":"projects/snowquery/#how-it-works","text":"For Snowflake: snowquery uses the reticulate package to run the official Snowflake Python connector in the background. This bypasses the need for users to manually configure ODBC drivers or deal with other R-specific Snowflake connection hurdles. The results come back as Pandas DataFrames from the Python connector which are seamlessly converted into R dataframes. For Redshift & PostgreSQL: For these databases, snowquery uses the standard DBI interface with the RPostgres driver, making for a normal and efficient connection method.","title":"How It Works"},{"location":"projects/snowquery/#key-features","text":"Simplified Querying: A single function, queryDB() , is the main entry point for executing queries. Credential Management: The package looks for a snowquery_creds.yaml file in the user's home directory ( ~ ). This file can store connection details (account, username, password, warehouse, role, etc.) for multiple named connections. Credentials can also be passed directly to the queryDB() function, overriding or supplementing the YAML file. Supported Databases: Snowflake, Redshift, and PostgreSQL. Python Dependency Management: For Snowflake connections, it checks for a Python installation and the snowflake-connector-python[pandas] library, providing guidance through error messages if they are missing. Connection Timeout: Users can specify a timeout for database connections. Kind of important if you want to run very long-running queries.","title":"Key Features"},{"location":"projects/snowquery/#technical-stack","text":"R Python (for Snowflake via reticulate ) snowflake-connector-python[pandas] DBI RPostgres yaml (for credential file parsing)","title":"Technical Stack"},{"location":"projects/snowquery/#installation-usage","text":"The package can be installed from CRAN or the development version from GitHub: # From CRAN install.packages(\"snowquery\") Typical usage would look like this: library(snowquery) # Using a named connection from snowquery_creds.yaml my_data <- queryDB(\"SELECT * FROM my_awesome_table\", conn_name = 'my_snowflake_dwh') # Manually passing credentials more_data <- queryDB( \"SELECT * FROM another_table\", db_type = 'postgres', host = 'localhost', port = 5432, database = 'mydb', username = 'user', password = 'password' )","title":"Installation &amp; Usage"},{"location":"projects/snowquery/#project-links-status","text":"Status: Active and available on CRAN. CRAN: https://cran.r-project.org/package=snowquery GitHub Repository: https://github.com/mermelstein/snowquery Package Website: https://snowquery.org","title":"Project Links &amp; Status"}]}